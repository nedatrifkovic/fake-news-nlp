# Fake News Detection (NLP Project)
## Overview
This project aims to classify news articles as **fake** or **real** using Natural Language Processing (NLP) techniques and machine learning models.  
It includes data preprocessing, feature extraction, model training, and evaluation.

## Tech Stack
- Python, uv
- pandas, scikit-learn
- nltk / spaCy
- gensim
- matplotlib, seaborn

## Project Structure
- `data/` → raw and processed datasets
- `models/` → trained ML models and vectorizers (structure tracked, files ignored)
  - `ml_models/` → trained machine learning models (.pkl files)
  - `vectorizers/` → TF-IDF vectorizer and Word2Vec models
- `notebooks/` → Jupyter notebook for EDA
- `src/` → main Python source code (preprocessing, features, models, training, prediction)
- `reports/` → results and visualizations

## Dataset
The dataset is **not included in this repository** due to size limitations.  
For download instructions, see step 2.1 in "Quick Start" above.

*Dataset last accessed: September 2025*  

For more details about the data directory structure, see [`data/README.md`](data/README.md).

## Quick Start

1. **Setup** (first time only):
   ```bash
   git clone https://github.com/yourusername/fake-news-nlp.git
   cd fake-news-nlp
   uv venv
   source .venv/bin/activate
   uv sync
   ```

2. **Prepare data** 
   1. Download raw data from Kaggle:
      - [Fake News Dataset (fake.csv)](https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection?select=fake.csv)  
      - [True News Dataset (true.csv)](https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection?select=true.csv)  
      - Place files in: `fake.csv` → `data/raw/`, `true.csv` → `data/raw/`
   
   2. Run the **EDA notebook** to generate the **interim dataset**:  

      ```bash
      # Create kernel for uv environment
      uv run python -m ipykernel install --user --name fake-news-nlp --display-name "Fake News NLP"
      
      # Run the notebook
      uv run jupyter notebook notebooks/eda.ipynb
      ```
      
      **Note**: 
      - Select the "Fake News NLP" kernel in Jupyter
      - Run all cells to generate `data/interim/interim.csv`
   
   3. Run the **preprocessing script** to generate the **processed dataset**:

      ```bash
      uv run python src/preprocessing.py
      ```
   
3. **Train models**:
   ```bash
   # Generate features and vectorizers
   uv run python src/features.py

   # Train ML models (add src to PYTHONPATH)
   PYTHONPATH=src uv run python src/train.py
   ```
   
   This will create trained models in `models/ml_models/` and vectorizers in `models/vectorizers/`.

4. **Make predictions**:

   The prediction script supports three different ways to make predictions:

   ### Option 1: Direct text input from command line
   ```bash
   uv run python src/predict.py \
   --text "Breaking news: Government announces new reforms." \
   --model logreg_tfidf.pkl \
   --feature tfidf
   ```

   ### Option 2: Predict from a text file
   Create a text file in `data/samples/` directory with one news text per line:
   ```bash
   # Create the samples directory if it doesn't exist
   mkdir -p data/samples

   # Create a sample file with multiple texts (one per line)
   echo "Stock markets rise after positive jobs report." > data/samples/sample_texts.txt
   echo "Scientists discover cure for aging in lab tests." >> data/samples/sample_texts.txt
   ```

   Then run prediction:
   ```bash
   uv run python src/predict.py \
   --input_file data/samples/sample_texts.txt \
   --model xgb_tfidf.pkl \
   --feature tfidf
   ```

   ### Option 3: Interactive input (no arguments)
   ```bash
   uv run python src/predict.py --model logreg_tfidf.pkl --feature tfidf
   ```
   The script will prompt you to enter the news text for prediction.

   ### Available Models and Features
   - **Models**: `logreg_tfidf.pkl`, `rf_w2v.pkl`, `xgb_tfidf.pkl`
   - **Features**: `tfidf`, `w2v`

## Models Folder

The `models/` folder contains trained machine learning models and vectorizers:

- **Directory structure is tracked** in git (via `.gitkeep` files)
- **Model files are ignored** due to size (they're generated by training scripts)
- **Subdirectories**:
  - `ml_models/` → Trained ML models (Logistic Regression, Random Forest, XGBoost)
  - `vectorizers/` → TF-IDF vectorizer and Word2Vec models

## Docker Support

You can also run this project using Docker:

### Quick Start with Docker

```bash
# 1. Build the Docker image
docker build -t fake-news-nlp .

# 2. Download datasets from Kaggle and place in data/raw/
#    - fake.csv → data/raw/
#    - true.csv → data/raw/

# 3. Run complete pipeline automatically
docker run -it --rm -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models fake-news-nlp ./setup.sh
```

**Important**: 
- `-v $(pwd)/data:/app/data` - mounts data folder for input/output
- `-v $(pwd)/models:/app/models` - mounts models folder to persist trained models
- If you get "no such file or directory" error, rebuild the Docker image:
```bash
docker build -t fake-news-nlp . --no-cache
```

### Manual Docker Commands

```bash
# Run individual steps
docker run -it --rm -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models fake-news-nlp uv run python src/preprocessing.py
docker run -it --rm -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models fake-news-nlp PYTHONPATH=src uv run python src/train.py

# Run Jupyter notebook in container
docker run -it --rm -p 8888:8888 -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models fake-news-nlp uv run jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root

# Run prediction
docker run -it --rm -v $(pwd)/data:/app/data -v $(pwd)/models:/app/models fake-news-nlp PYTHONPATH=src uv run python src/predict.py --text "Your news text here" --model logreg_tfidf.pkl --feature tfidf
```

### Automated Setup Scripts

```bash
# Make scripts executable
chmod +x setup.sh docker-setup.sh

# Run local setup (after downloading data)
./setup.sh

# Run Docker setup
./docker-setup.sh
```

### Using Makefile (Recommended)

```bash
# See all available commands
make help

# Install dependencies
make install

# Run complete pipeline (after downloading data)
make setup

# Docker commands
make docker-build
make docker-run
make docker-setup

# Clean generated files
make clean

# Test prediction
make test
```

## Documentation

For more details about the project structure and data flow, see our documentation:

- [Data Pipeline](docs/data_pipeline.md)

